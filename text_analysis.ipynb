{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 404: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ for 11668.0 not found. Skipping...\n",
      "Error 404: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ for 17671.4 not found. Skipping...\n"
     ]
    }
   ],
   "source": [
    "#DATA EXTRACTION\n",
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, HTTPError\n",
    "import ssl\n",
    "import gzip\n",
    "#Ignore ssl certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE0\n",
    "\n",
    "workbook = openpyxl.load_workbook(\"Input.xlsx\")\n",
    "sheet = workbook[\"Sheet1\"]\n",
    "#Accessing each rows\n",
    "for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row, min_col=1, max_col=2):\n",
    "    url_id_cell, url_cell = row\n",
    "    url_id = url_id_cell.value\n",
    "    url = url_cell.value\n",
    "    try:\n",
    "        with urlopen(url) as response:\n",
    "            if response.info().get('Content-Encoding') == 'gzip':\n",
    "                with gzip.GzipFile(fileobj=response) as gzipped_response:\n",
    "                    html_content = gzipped_response.read().decode('utf-8')\n",
    "            else:\n",
    "                html_content = response.read().decode('utf-8')\n",
    "\n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            print(f\"Error 404: {url} for {url_id} not found. Skipping...\")\n",
    "        continue\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "    head_tag = soup.head\n",
    "    title_tag = head_tag.title\n",
    "    try:\n",
    "        title_text = title_tag.text\n",
    "    except:\n",
    "        title_text = \"No Title\"\n",
    "        print(f\"No title for {url_id}\")\n",
    "        continue\n",
    "    article_div = soup.find(\"div\", class_=\"td-post-content tagdiv-type\")\n",
    "    if article_div:\n",
    "        content = '\\n'.join(paragraph.text for paragraph in article_div.find_all('p'))\n",
    "    with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{title_text}\\n\")\n",
    "        #file.write(\"\\n\")\n",
    "        file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Cleaning using stop words\n",
    "stop_words_list = []\n",
    "for filename in os.listdir(\"StopWords\"):\n",
    "    file_path = os.path.join(\"StopWords\", filename)\n",
    "    with open(file_path, \"r\")  as file:\n",
    "        for word in file:\n",
    "            stop_words_list.extend(word.strip().split())\n",
    "for item in stop_words_list:\n",
    "    if item == \"|\":\n",
    "        stop_words_list.remove(item)\n",
    "master_dict_cleaned = {\"positive_words\" : [], \"negative_words\":[]}\n",
    "for file_desig in os.listdir(\"MasterDictionary\"):\n",
    "    path_file = os.path.join(\"MasterDictionary\", file_desig)\n",
    "    if file_desig == \"negative-words.txt\":\n",
    "        with open(path_file, \"r\") as files:\n",
    "            for word in files:\n",
    "                if word.strip() in stop_words_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    master_dict_cleaned[\"negative_words\"].append(word.strip())\n",
    "    elif file_desig == \"positive-words.txt\":\n",
    "        with open(path_file, \"r\") as files:\n",
    "            for word in files:\n",
    "                if word.strip() in stop_words_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    master_dict_cleaned[\"positive_words\"].append(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\idakw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\idakw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\idakw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Full text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import openpyxl\n",
    "import string\n",
    "import pandas as pd\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"cmudict\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def count_syllables(word):\n",
    "    try:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmudict.dict()[word.lower()]])\n",
    "    except KeyError:\n",
    "        # If the word is not in the CMU Pronouncing Dictionary, return 0\n",
    "        return 0\n",
    "def find_complex_words(text):\n",
    "    complex_words = [word for word in text if count_syllables(word) >= 2]\n",
    "    return complex_words\n",
    "def count_syllables(word):\n",
    "    # Count the number of vowels in a word\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    # Handle exceptions for words ending with \"es\" and \"ed\"\n",
    "    if word.endswith((\"es\", \"ed\")):\n",
    "        return count\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            count += 1\n",
    "    return count\n",
    "def count_syllables_in_text(text):\n",
    "    # Count syllables for each word and sum them up\n",
    "    total_syllables = sum(count_syllables(word) for word in text)\n",
    "    return total_syllables\n",
    "       \n",
    "def count_personal_pronouns(words):\n",
    "    # Define a regex pattern for personal pronouns\n",
    "    pronoun_pattern = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "\n",
    "    # Exclude instances where \"US\" refers to the country\n",
    "    country_pattern = r'\\bUS\\b'\n",
    "\n",
    "    # Join the list of words into a string for regex matching\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    # Use regex to find matches for personal pronouns\n",
    "    pronoun_matches = re.findall(pronoun_pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Use regex to find matches for the country name \"US\"\n",
    "    country_matches = re.findall(country_pattern, text)\n",
    "\n",
    "    # Exclude instances of \"US\" referring to the country from personal pronoun matches\n",
    "    filtered_pronoun_matches = [pronoun.lower() for pronoun in pronoun_matches if pronoun.lower() not in country_matches]\n",
    "\n",
    "    # Count the number of personal pronoun matches\n",
    "    count = len(filtered_pronoun_matches)\n",
    "\n",
    "    return count\n",
    "#progress_report = 1\n",
    "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "for filename in os.listdir(\"text_files\"):\n",
    "    file_path = os.path.join(\"text_files\", filename)\n",
    "    with open(file_path, \"r\", encoding= \"utf-8\") as file_hnd:\n",
    "        text = file_hnd.read()\n",
    "        #Using nltk tokenization as required in Text Analysis.docx\n",
    "        tokens = word_tokenize(text)\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        #1: SENTIMENTAL ANALYSIS\n",
    "        cleaned_words = [word for word in tokens if word.isalpha() and word not in stop_words_list]\n",
    "        for item in cleaned_words:\n",
    "            if item in master_dict_cleaned[\"positive_words\"] :\n",
    "                positive_score+=1\n",
    "            elif item in master_dict_cleaned[\"negative_words\"] :\n",
    "                negative_score+=1\n",
    "            else:\n",
    "                continue\n",
    "        polarity_score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score)/ ((len(cleaned_words)) + 0.000001)\n",
    "        #2: ANALYSIS OF READABILITY\n",
    "        sentences = text.split(\".\")\n",
    "        wds = text.split()\n",
    "        avg_sentence_length = len(wds)/ len(sentences)\n",
    "        complx_words = find_complex_words(wds)\n",
    "        percentage_complx_word = len(complx_words)/len(wds)\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complx_word)\n",
    "        #3: Average Number of Words per sentence\n",
    "        avg_words_sentence = len(wds)/ len(sentences)\n",
    "        #4: Complex word count\n",
    "        complx_words_count = len(complx_words)\n",
    "        #5: Word count per the required nltk stopwords usage\n",
    "        nltk_stop_words = set(stopwords.words('english'))\n",
    "        nltk_cleaned_words = [word for word in tokens if word.lower() not in nltk_stop_words]\n",
    "        word_count = len(nltk_cleaned_words)\n",
    "        #6: Syllable count per word\n",
    "        syllable_per_word = (count_syllables_in_text(wds))/ len(wds)\n",
    "        #7: Personal Pronouns\n",
    "        personal_prons = count_personal_pronouns(wds)\n",
    "        #8: Average word length\n",
    "        total_characters = sum(len(list(wd)) for wd in wds)\n",
    "        #print(total_characters, len(wds))\n",
    "        avg_word_length = total_characters / len(wds)\n",
    "        #Create a dictionary of the things we want\n",
    "        output_data = {\"POSITIVE SCORE\": positive_score, \"NEGATIVE SCORE\": negative_score, \"POLARITY SCORE\": polarity_score, \n",
    "                       \"SUBJECTIVITY SCORE\": subjectivity_score, \"AVG SENTENCE LENGTH\": avg_sentence_length, \n",
    "                       \"PERCENTAGE OF COMPLEX WORDS\": percentage_complx_word, \"FOG INDEX\": fog_index, \n",
    "                       \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_sentence, \"COMPLEX WORD COUNT\": complx_words_count, \n",
    "                       \"WORD COUNT\": word_count, \"SYLLABLE PER WORD\": syllable_per_word, \"PERSONAL PRONOUNS\": personal_prons,\n",
    "                       \"AVG WORD LENGTH\": avg_word_length}\n",
    "        \n",
    "        url_id = float(filename.replace(\".txt\",\"\"))\n",
    "        filtered_df = output_df[~output_df[\"URL_ID\"].isin([11668.0, 17671.4])]\n",
    "        for i, row in filtered_df.iterrows():\n",
    "            if url_id == row[\"URL_ID\"]:\n",
    "                # If the URL_ID matches, update the values in output_df for each key in output_data\n",
    "                for key, value in output_data.items():\n",
    "                    output_df.at[i, key] = value\n",
    "    #print(progress_report)\n",
    "    #progress_report +=1\n",
    "output_df.to_excel(\"Output Data Structure.xlsx\", index=False)\n",
    "#pd.DataFrame(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11668.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-neural-ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "24  11668.0  https://insights.blackcoffer.com/how-neural-ne...   \n",
       "\n",
       "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "24             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "24                  NaN                          NaN        NaN   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "24                               NaN                 NaN         NaN   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "24                NaN                NaN              NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = output_df[output_df[\"URL_ID\"] == 11668.0]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
